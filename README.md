# CierraPhone - OpenAI Speech-to-Speech Integration


This repository showcases the integration between **Agent Voice Response** and **OpenAI's Real-time Speech-to-Speech API**. The application leverages OpenAI's powerful language model to process audio input from users, providing intelligent, context-aware responses in real-time audio format.

## Prerequisites

To set up and run this project, you will need:

1. **Node.js** and **npm** installed
2. An **OpenAI API key** with access to the real-time API
3. **WebSocket** support in your environment

## Setup

### 1. Clone the Repository

```bash
git clone https://github.com/cierrateam/avr-sts-openai.git
cd avr-sts-openai
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Configure Environment Variables

Create a `.env` file in the root of the project to store your API keys and configuration. You will need to add the following variables:

```bash
OPENAI_API_KEY=your_openai_api_key
PORT=6030
OPENAI_MODEL=gpt-4o-realtime-preview  # Optional, defaults to gpt-4o-realtime-preview

# Agent configuration
AGENT_ID=your_agent_id  # Required: Agent ID for loading instructions from the API

OPENAI_TEMPERATURE=0.8  # Optional, controls randomness (0.0-1.0), defaults to 0.8
OPENAI_MAX_TOKENS=100  # Optional, controls response length, defaults to "inf"
```

Replace `your_openai_api_key` with your actual OpenAI API key.

### 4. Running the Application

Start the application by running the following command:

```bash
node index.js
```

The server will start on the port defined in the environment variable (default: 6030).

## How It Works

The **Agent Voice Response** system integrates with OpenAI's Real-time Speech-to-Speech API to provide intelligent audio-based responses to user queries. The server receives audio input from users, forwards it to OpenAI's API, and then returns the model's response as audio in real-time using WebSocket communication.

### Key Components

- **Express.js Server**: Handles incoming audio streams from clients
- **WebSocket Communication**: Manages real-time communication with OpenAI's API
- **Audio Processing**: Handles audio format conversion between 8kHz and 24kHz
- **Real-time Streaming**: Processes and streams audio data in real-time

### Audio Processing

The application includes two main audio processing functions:

1. **Upsampling (8kHz to 24kHz)**:
   - Converts client audio from 8kHz to 24kHz using linear interpolation
   - Required for OpenAI's API which expects 24kHz input

2. **Downsampling (24kHz to 8kHz)**:
   - Converts OpenAI's 24kHz output back to 8kHz
   - Ensures compatibility with client audio systems

## API Endpoints

### POST `/speech-to-speech-stream`

This endpoint accepts an audio stream and returns a streamed audio response generated by OpenAI.

**Request:**
- Content-Type: audio/x-raw
- Format: 16-bit PCM at 8kHz
- Method: POST

**Response:**
- Content-Type: text/event-stream
- Format: 16-bit PCM at 8kHz
- Streamed audio data in real-time

## Customizing the Application

### Environment Variables

You can customize the application behavior using the following environment variables:

- `OPENAI_API_KEY`: Your OpenAI API key (required)
- `PORT`: The port on which the server will listen (default: 6030)
- `OPENAI_MODEL`: The OpenAI model to use (default: gpt-4o-realtime-preview)
- `AGENT_ID`: Agent ID for loading instructions from the API (required)
- `OPENAI_TEMPERATURE`: Controls randomness in responses (0.0-1.0, default: 0.8)
- `OPENAI_MAX_TOKENS`: Controls the maximum length of the response (default: "inf")

### Instruction Loading

The application loads AI instructions from a centralized API endpoint using the `AGENT_ID` environment variable.

#### Agent-Based Instruction Loading
The application fetches instructions from the agent API endpoint:

```bash
AGENT_ID=your_agent_id
```

The application will make a GET request to:
```
https://test-micha.pbx.cierra.ai/api/agents/{AGENT_ID}/system-instructions
```

The API should return a JSON response containing the instructions. The application will look for the instructions in the following fields (in order of priority):
- `system` - Primary field for system instructions
- `instructions` - Alternative field name
- The response data itself (if it's a string)

The application will include the session UUID in the request headers as `X-AVR-UUID` for personalized instructions.

#### Fallback Behavior
If the `AGENT_ID` is not provided or the API request fails, the application will fall back to default instructions:
```
"You are a helpful assistant that can answer questions and help with tasks."
```

This centralized approach ensures consistent instruction management across all agent deployments.

## Error Handling

The application includes comprehensive error handling for:
- WebSocket connection issues
- Audio processing errors
- OpenAI API errors
- Stream processing errors

All errors are logged to the console and appropriate error messages are returned to the client.


## License

MIT License - see the [LICENSE](LICENSE.md) file for details.
